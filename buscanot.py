import asyncio
import aiohttp
import time
import json
import re
import random
import unicodedata
from pathlib import Path
from datetime import datetime, date, timedelta
from typing import Dict, List, Any, Optional, Tuple
from urllib.parse import urljoin, urlparse, parse_qsl, urlencode, urlunparse, quote_plus

import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup, SoupStrainer
from aiohttp import ClientTimeout
from email.utils import parsedate_to_datetime
import urllib.robotparser as rp

# =========================
# Configuraci√≥n general
# =========================
st.set_page_config(page_title="BuscaNot", layout="wide")
st.title("üåç Buscador de noticias")

DB_PATH = Path("media_db.json")

BASE_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "es-ES,es;q=0.9",
    "Connection": "keep-alive",
}

DEFAULT_TIMEOUT = ClientTimeout(total=20, connect=10, sock_connect=10, sock_read=15)
ALLOWED_SCHEMES = {"http", "https"}
TRACKING_PARAMS = {"utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content", "gclid", "fbclid", "mc_cid", "mc_eid"}

# =========================
# Patrones comunes para descubrir hemeroteca (si no se define)
# =========================
COMMON_ARCHIVE_PATTERNS = [
    "/hemeroteca/{yyyy}-{mm}-{dd}/",      
    "/hemeroteca/{yyyy}/{mm}/{dd}/",     
    "/archivo/{yyyy}-{mm}-{dd}/",
    "/archivo/{yyyy}/{mm}/{dd}/",
    "/{yyyy}/{mm}/{dd}/",                
]

# =========================
# Descubridor autom√°tico de patrones de hemeroteca
# =========================

DATE_REGEXES = [
    re.compile(r"(19|20)\d{2}[/-]\d{1,2}[/-]\d{1,2}"),
    re.compile(r"/(19|20)\d{2}/\d{1,2}/\d{1,2}/"),
]


def is_same_domain(url: str, base_url: str) -> bool:
    try:
        u = urlparse(url)
        b = urlparse(base_url)
        # Permitimos paths relativos (netloc vac√≠o) o mismo dominio
        return u.netloc == "" or u.netloc == b.netloc
    except Exception:
        return False


async def collect_candidate_date_urls(
    session: aiohttp.ClientSession,
    source: Dict[str, Any],
    headers: Dict[str, str],
    timeout: ClientTimeout,
    ttl_sec: int,
    neg_ttl_sec: int,
    respect_robots: bool,
) -> List[str]:
    base_url = source.get("base_url") or source.get("url")
    if not base_url:
        return []

    html = await fetch_html(
        session,
        base_url,
        headers=headers,
        timeout=timeout,
        ttl_sec=ttl_sec,
        neg_ttl_sec=neg_ttl_sec,
        respect_robots=respect_robots,
    )
    if not html:
        return []

    soup = BeautifulSoup(html, "html.parser")
    candidates: set[str] = set()

    for a in soup.find_all("a", href=True):
        href = (a["href"] or "").strip()
        if not href:
            continue

        abs_url = urljoin(base_url, href)
        if not is_same_domain(abs_url, base_url):
            continue

        path = urlparse(abs_url).path or "/"
        if any(rx.search(path) for rx in DATE_REGEXES):
            candidates.add(abs_url)

    return list(candidates)


def normalize_segment(seg: str) -> str:
    # 4 d√≠gitos -> candidato a a√±o
    if re.fullmatch(r"\d{4}", seg):
        year = int(seg)
        if 1900 <= year <= 2100:
            return "{yyyy}"

    # 1‚Äì2 d√≠gitos -> mes o d√≠a
    if re.fullmatch(r"\d{1,2}", seg):
        n = int(seg)
        if 1 <= n <= 12:
            return "{mm}"
        if 1 <= n <= 31:
            return "{dd}"

    return seg


def make_path_template(path: str) -> str:
    """
    /archivo/2025/11/07/politica/foo.html
    -> /archivo/{yyyy}/{mm}/{dd}/politica/foo.html
    """
    def replace_inline_dates(seg: str) -> str:
        # Detecta 2025-11-07 o 2025/11/07 dentro del segmento
        m = re.search(r"(19|20)\d{2}[-/]\d{1,2}[-/]\d{1,2}", seg)
        if not m:
            return seg
        date_str = m.group(0)
        parts = re.split("[-/]", date_str)
        if len(parts) == 3:
            yyyy, mm, dd = parts
            normalized = "{yyyy}-{mm}-{dd}"
            return seg.replace(date_str, normalized)
        return seg

    path = path or "/"
    segments = path.strip("/").split("/")
    new_segments: List[str] = []

    for seg in segments:
        seg = replace_inline_dates(seg)
        if "{yyyy}" in seg:
            # Ya se normaliz√≥ inline (caso 2025-11-07)
            new_segments.append(seg)
            continue
        new_segments.append(normalize_segment(seg))

    return "/" + "/".join(new_segments)


def choose_best_archive_pattern(candidate_urls: List[str]) -> Optional[str]:
    """
    A partir de URLs con fechas, elige la plantilla m√°s coherente y
    devuelve un patr√≥n de hemeroteca tipo "/archivo/{yyyy}/{mm}/{dd}/".
    """
    if not candidate_urls:
        return None

    groups: Dict[str, List[str]] = {}
    for url in candidate_urls:
        path = urlparse(url).path
        template = make_path_template(path)
        groups.setdefault(template, []).append(url)

    def has_date_placeholders(t: str) -> bool:
        return "{yyyy}" in t and "{mm}" in t and "{dd}" in t

    scored: List[Tuple[int, str]] = []
    for tmpl, urls in groups.items():
        if not has_date_placeholders(tmpl):
            continue

        score = len(urls)  # cu√°ntas URLs encajan en esta plantilla

        # Bonus si la ruta contiene palabras t√≠picas de archivo
        if re.search(r"(archivo|hemeroteca|archive)", tmpl, re.IGNORECASE):
            score += 5

        scored.append((score, tmpl))

    if not scored:
        return None

    scored.sort(reverse=True)
    best_template = scored[0][1]

    # Recortamos hasta el d√≠a: /archivo/{yyyy}/{mm}/{dd}/...
    parts = best_template.split("{yyyy}")
    prefix = parts[0]

    archive_pattern = prefix + "{yyyy}/{mm}/{dd}/"
    return archive_pattern


async def smart_auto_detect_archive_pattern(
    session: aiohttp.ClientSession,
    source: Dict[str, Any],
    headers: Dict[str, str],
    timeout: ClientTimeout,
    ttl_sec: int,
    neg_ttl_sec: int,
    respect_robots: bool,
) -> Optional[str]:
    """
    Pipeline "listo": mira enlaces reales del sitio, detecta fechas en paths
    y propone un archive_pattern date-based.
    """
    candidate_urls = await collect_candidate_date_urls(
        session=session,
        source=source,
        headers=headers,
        timeout=timeout,
        ttl_sec=ttl_sec,
        neg_ttl_sec=neg_ttl_sec,
        respect_robots=respect_robots,
    )

    if not candidate_urls:
        return None

    archive_pattern = choose_best_archive_pattern(candidate_urls)
    return archive_pattern


async def auto_detect_archive_pattern_for_source(
    session: aiohttp.ClientSession,
    source: Dict[str, Any],
    headers: Dict[str, str],
    timeout: ClientTimeout,
    ttl_sec: int,
    neg_ttl_sec: int,
    respect_robots: bool,
) -> Optional[str]:
    """
    Fallback sencillo: prueba los COMMON_ARCHIVE_PATTERNS cl√°sicos con archive_selector.
    """
    selector_archive = source.get("archive_selector")
    if not selector_archive:
        return None

    base_url = source.get("base_url") or source.get("url")
    if not base_url:
        return None

    today = date.today()

    for patt in COMMON_ARCHIVE_PATTERNS:
        test_url = urljoin(
            base_url,
            patt.format(
                yyyy=today.strftime("%Y"),
                mm=today.strftime("%m"),
                dd=today.strftime("%d"),
            ),
        )

        html = await fetch_html(
            session,
            test_url,
            headers=headers,
            timeout=timeout,
            ttl_sec=ttl_sec,
            neg_ttl_sec=neg_ttl_sec,
            respect_robots=respect_robots,
        )
        if not html:
            continue

        try:
            soup = BeautifulSoup(html, "html.parser")
            elements = soup.select(selector_archive)
        except Exception:
            continue

        if elements:
            full_pattern = urljoin(base_url, patt)
            return full_pattern

    return None
# =========================
# Persistencia BD
# =========================
def save_db(db: Dict[str, List[Dict[str, Any]]]) -> None:
    try:
        with open(DB_PATH, "w", encoding="utf-8") as f:
            json.dump(db, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"‚ùå No se pudo guardar media_db.json: {e}")

def load_db() -> Dict[str, List[Dict[str, Any]]]:
    if DB_PATH.exists():
        try:
            with open(DB_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, dict):
                return data
            else:
                st.error("‚ö†Ô∏è El fichero media_db.json no contiene un objeto JSON de nivel ra√≠z (deber√≠a ser { ... }).")
                return {}
        except Exception as e:
            st.error(f"‚ö†Ô∏è No se pudo leer media_db.json: {e}")
            return {}
    else:
        st.error("‚ö†Ô∏è No se encontr√≥ el fichero media_db.json en el directorio de la app. Cr√©alo y vuelve a ejecutar.")
        return {}

# =========================
# Cach√©s
# =========================
@st.cache_resource
def get_html_cache() -> Dict[str, Tuple[float, str]]:
    return {}

@st.cache_resource
def get_neg_cache() -> Dict[str, float]:
    return {}

@st.cache_resource
def get_translation_cache() -> Dict[Tuple[str, str], str]:
    """(texto, lang_destino) -> traducci√≥n"""
    return {}

def cache_get(url: str, ttl_sec: int) -> Optional[str]:
    cache = get_html_cache()
    item = cache.get(url)
    if not item:
        return None
    ts, html = item
    if (time.time() - ts) <= ttl_sec:
        return html
    cache.pop(url, None)
    return None

def cache_put(url: str, html: str) -> None:
    cache = get_html_cache()
    cache[url] = (time.time(), html)

def neg_cache_hit(url: str, neg_ttl_sec: int) -> bool:
    neg = get_neg_cache()
    ts = neg.get(url)
    if not ts:
        return False
    if (time.time() - ts) <= neg_ttl_sec:
        return True
    neg.pop(url, None)
    return False

def neg_cache_put(url: str) -> None:
    neg = get_neg_cache()
    neg[url] = time.time()

# =========================
# Utilidades
# =========================
def build_headers(country: str) -> Dict[str, str]:
    h = dict(BASE_HEADERS)
    if country in {"Espa√±a", "Andorra"}:
        h["Accept-Language"] = "es-ES,es;q=0.9,*;q=0.1"
    elif country in {"Portugal"}:
        h["Accept-Language"] = "pt-PT,pt;q=0.9,*;q=0.1"
    elif country in {"Francia"}:
        h["Accept-Language"] = "fr-FR,fr;q=0.9,*;q=0.1"
    elif country in {"Alemania"}:
        h["Accept-Language"] = "de-DE,de;q=0.9,*;q=0.1"
    else:
        h["Accept-Language"] = "*;q=0.1"
    return h

def norm_text(s: str) -> str:
    s = (s or "").strip().lower()
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"\s+", " ", s)
    return s

def absolutize(link: str, base_url: Optional[str]) -> str:
    if not link:
        return ""
    full = urljoin(base_url or "", link)
    parsed = urlparse(full)
    if parsed.scheme not in ALLOWED_SCHEMES:
        return ""
    qs = [(k, v) for k, v in parse_qsl(parsed.query, keep_blank_values=True) if k not in TRACKING_PARAMS]
    cleaned = parsed._replace(query=urlencode(qs))
    return urlunparse(cleaned)

def extract_time_candidate(el) -> Optional[str]:
    t = el.find("time")
    if t and (t.get("datetime") or t.get_text(strip=True)):
        return t.get("datetime") or t.get_text(strip=True)
    for attr in ("data-published", "data-date", "data-time", "aria-label"):
        v = el.get(attr)
        if v:
            return v
    return None

def is_latin_lang(lang: str) -> bool:
    return (lang or "").lower() in {"es","fr","pt","de","en","it","ca","gl"}

def split_terms(terms: str) -> List[Tuple[str, bool]]:
    if not terms:
        return []
    # Captura frases entre comillas
    quoted = re.findall(r'"([^"]+)"|\'([^\']+)\'', terms)
    exacts = {q[0] or q[1] for q in quoted if (q[0] or q[1])}
    # Elimina las frases exactas del resto
    tmp = terms
    for q in exacts:
        tmp = tmp.replace(f'"{q}"', " ").replace(f"'{q}'", " ")
    # Resto por separadores
    others = [t.strip() for t in re.split(r"[,\n;]+", tmp) if t.strip()]
    out: List[Tuple[str,bool]] = []
    out += [(t, True) for t in exacts]
    out += [(t, False) for t in others]
    return out

def build_regex_from_terms(terms: List[Tuple[str,bool]], whole_words: bool, ignore_case: bool) -> Optional[re.Pattern]:
    if not terms:
        return None
    parts: List[str] = []
    for text, is_exact in terms:
        esc = re.escape(text)
        if is_exact:
            parts.append(f"(?:{esc})")
        else:
            if whole_words:
                parts.append(rf"(?<!\w)(?:{esc})(?!\w)")
            else:
                parts.append(f"(?:{esc})")
    body = "|".join(parts)
    flags = re.IGNORECASE if ignore_case else 0
    try:
        return re.compile(body, flags)
    except re.error as e:
        st.error(f"Error en la expresi√≥n regular: {e}")
        return None

def compute_relevance_score_from_terms(
    title: str,
    include_terms: List[Tuple[str, bool]],
    exclude_terms: List[Tuple[str, bool]],
) -> int:
    if not title:
        return 0
    text = norm_text(title)
    score = 0

    for term, _ in include_terms:
        t = norm_text(term)
        if t and t in text:
            score += 1

    for term, _ in exclude_terms:
        t = norm_text(term)
        if t and t in text:
            score -= 2

    return score

# =========================
# Traducci√≥n (Google Translate endpoint p√∫blico)
# =========================
async def translate_text(session: aiohttp.ClientSession, text: str, target_lang: str) -> str:
    """
    Usa endpoint p√∫blico de Google Translate (sin API key).
    Retorna texto traducido (top-1).
    """
    if not text or not target_lang:
        return text
    target_lang = target_lang.lower()
    cache = get_translation_cache()
    key = (text, target_lang)
    if key in cache:
        return cache[key]
    try:
        params = {
            "client": "gtx",
            "sl": "auto",
            "tl": target_lang,
            "dt": "t",
            "q": text,
        }
        url = "https://translate.googleapis.com/translate_a/single"
        async with session.get(url, params=params, timeout=DEFAULT_TIMEOUT, ssl=True, headers={"User-Agent": BASE_HEADERS["User-Agent"]}) as r:
            r.raise_for_status()
            data = await r.json(content_type=None)
        # data[0] es lista de segmentos; cada segmento [trad, orig, ...]
        translated_segments = [seg[0] for seg in data[0] if seg and seg[0]]
        out = "".join(translated_segments).strip()
        if out:
            cache[key] = out
            return out
    except Exception:
        # fallo silencioso: devolvemos original
        return text
    return text

async def translate_terms_list(session: aiohttp.ClientSession, terms: List[Tuple[str,bool]], target_lang: str) -> List[Tuple[str,bool]]:
    out: List[Tuple[str,bool]] = []
    for t, ex in terms:
        tt = await translate_text(session, t, target_lang)
        if tt and tt.lower() != t.lower():
            out.append((tt, ex))
    return out

async def prepare_terms_per_language(
    session: aiohttp.ClientSession,
    include_terms_raw: str,
    exclude_terms_raw: str,
    languages: List[str],
    user_whole_words: bool,
    ignore_case: bool,
    enable_translation: bool,
) -> Dict[str, Dict[str, Any]]:

    base_inc = split_terms(include_terms_raw)
    base_exc = split_terms(exclude_terms_raw)

    result: Dict[str, Dict[str, Any]] = {}

    for lang in languages:
        lang = (lang or "").lower() or "es"

        # empezamos con los originales
        inc_terms = list(base_inc)
        exc_terms = list(base_exc)

        # si el idioma no es "sin traducir", intentamos traducir
        if enable_translation and lang not in {"", "es"}:
            inc_trans = await translate_terms_list(session, base_inc, lang)
            exc_trans = await translate_terms_list(session, base_exc, lang)

            def merge(a, b):
                seen = set()
                out = []
                for t in a + b:
                    key = (t[0].lower(), t[1])
                    if key not in seen:
                        seen.add(key)
                        out.append(t)
                return out

            inc_terms = merge(inc_terms, inc_trans)
            exc_terms = merge(exc_terms, exc_trans)

        effective_whole_words = user_whole_words and is_latin_lang(lang)

        inc_re = build_regex_from_terms(inc_terms, whole_words=effective_whole_words, ignore_case=ignore_case) if inc_terms else None
        exc_re = build_regex_from_terms(exc_terms, whole_words=effective_whole_words, ignore_case=ignore_case) if exc_terms else None

        result[lang] = {
            "include_terms": inc_terms,
            "exclude_terms": exc_terms,
            "include_re": inc_re,
            "exclude_re": exc_re,
        }

    return result
    
# =========================
# Respetar robots (cach√© parsers)
# =========================
@st.cache_resource
def get_robot_cache():
    return {}

async def is_allowed(session: aiohttp.ClientSession, headers: Dict[str, str], site_url: str, path: str = "/") -> bool:
    try:
        parsed = urlparse(site_url)
        base = f"{parsed.scheme}://{parsed.netloc}"
        robots_url = urljoin(base, "/robots.txt")
        cache = get_robot_cache()
        parser = cache.get(robots_url)
        if not parser:
            async with session.get(robots_url, headers=headers, timeout=DEFAULT_TIMEOUT, ssl=True) as r:
                if r.status != 200:
                    return True
                txt = await r.text()
            parser = rp.RobotFileParser()
            parser.parse(txt.splitlines())
            cache[robots_url] = parser
        return parser.can_fetch(headers.get("User-Agent", BASE_HEADERS["User-Agent"]), path or "/")
    except Exception:
        return True

# =========================
# Ayuda hemeroteca
# =========================
def iter_archive_urls(source: Dict[str, Any], start_date: date, end_date: date, day_cap: int = 31) -> List[str]:
    patt = source.get("archive_pattern")
    if not patt:
        return []
    urls: List[str] = []
    d = start_date
    count = 0
    while d <= end_date and count < day_cap:
        urls.append(patt.format(yyyy=d.strftime("%Y"), mm=d.strftime("%m"), dd=d.strftime("%d")))
        d += timedelta(days=1)
        count += 1
    return urls

def daterange(start: date, end: date):
    d = start
    while d <= end:
        yield d
        d += timedelta(days=1)

def iter_archive_urls_for_dates(source: Dict[str, Any], dates: List[date]) -> List[str]:
    patt = source.get("archive_pattern")
    if not patt:
        return []
    urls: List[str] = []
    for d in dates:
        urls.append(patt.format(yyyy=d.strftime("%Y"), mm=d.strftime("%m"), dd=d.strftime("%d")))
    return urls

def is_date_based_pattern(pattern: Optional[str]) -> bool:
    if not pattern:
        return False
    return any(tag in pattern for tag in ["{yyyy}", "{mm}", "{dd}"])

def is_page_based_pattern(pattern: Optional[str]) -> bool:
    if not pattern:
        return False
    return "{page}" in pattern

def iter_archive_urls_for_pages(source: Dict[str, Any], max_pages: int = 10) -> List[str]:
    pattern = source.get("archive_pattern")
    if not pattern or not is_page_based_pattern(pattern):
        return []
    return [pattern.format(page=i) for i in range(1, max_pages + 1)]

# =========================
# Networking as√≠ncrono + RSS/HTML
# =========================
from functools import lru_cache

async def fetch_html(
    session: aiohttp.ClientSession,
    url: str,
    headers: Dict[str, str],
    timeout: ClientTimeout,
    ttl_sec: int,
    neg_ttl_sec: int,
    respect_robots: bool,
) -> str:
    # ‚úÖ 1) Cache tradicional (disco o memoria persistente)
    if neg_cache_hit(url, neg_ttl_sec):
        return ""

    cached = cache_get(url, ttl_sec)
    if cached is not None:
        return cached

    # ‚úÖ 2) Respeto a robots.txt (si aplica)
    if respect_robots:
        parsed = urlparse(url)
        if not await is_allowed(session, headers, f"{parsed.scheme}://{parsed.netloc}", parsed.path):
            st.session_state.setdefault("logs", []).append(f"ü§ñ Robots bloquea {url}")
            neg_cache_put(url)
            return ""

    # ‚úÖ 3) Intentos de descarga con reintentos exponenciales
    tries = 3
    for attempt in range(1, tries + 1):
        try:
            async with session.get(url, headers=headers, timeout=timeout, ssl=True) as resp:
                resp.raise_for_status()
                html = await resp.text()

                # Guarda en cach√©
                cache_put(url, html)

                return html

        except Exception as e:
            if attempt == tries:
                neg_cache_put(url)
                st.session_state.setdefault("logs", []).append(f"‚ùå GET {url}: {e}")
                return ""

            await asyncio.sleep((2 ** (attempt - 1)) + random.random())

async def fetch_google_news_rss_for_source(
    session: aiohttp.ClientSession,
    source: Dict[str, Any],
    include_terms_raw: str,
    headers: Dict[str, str],
    timeout: ClientTimeout,
    recent_window_days: int = 7,
) -> Optional[List[Tuple[str, str, Optional[datetime]]]]:
    base = source.get("base_url") or source.get("url")
    if not base:
        return None

    if not include_terms_raw or not include_terms_raw.strip():
        return None

    parsed = urlparse(base)
    domain = parsed.netloc
    if not domain:
        return None

    query = f"{include_terms_raw.strip()} site:{domain} when:{recent_window_days}d"
    q_param = quote_plus(query)

    feed_url = f"https://news.google.com/rss/search?q={q_param}"

    # Reutilizamos el parser gen√©rico de RSS que ya tienes
    try:
        rss = await try_fetch_rss(
            session=session,
            page_url=feed_url,
            headers=headers,
            timeout=timeout,
            rss_url=feed_url,
        )
        return rss
    except Exception as e:
        st.session_state.setdefault("logs", []).append(
            f"‚ö†Ô∏è {source.get('name','¬ømedio?')}: fallo al consultar Google News RSS ({e})"
        )
        return None
        
async def try_fetch_rss(
    session: aiohttp.ClientSession,
    page_url: str,
    headers: Dict[str, str],
    timeout: ClientTimeout,
    rss_url: Optional[str] = None,
) -> Optional[List[Tuple[str, str, Optional[datetime]]]]:
    async def fetch_and_parse(feed_url: str) -> Optional[List[Tuple[str, str, Optional[datetime]]]]:
        try:
            async with session.get(feed_url, headers=headers, timeout=timeout, ssl=True) as r2:
                if r2.status != 200:
                    return None
                xml = await r2.text()
            feed = BeautifulSoup(xml, "xml")
            items = feed.find_all(["item", "entry"])
            out: List[Tuple[str, str, Optional[datetime]]] = []
            for it in items:
                title = (it.title.get_text(strip=True) if it.title else "").strip()
                link = (
                    it.link.get("href")
                    if it.link and it.link.has_attr("href")
                    else (it.link.get_text(strip=True) if it.link else "")
                ).strip()
                pub = None
                if it.find("pubDate"):
                    try:
                        pub = parsedate_to_datetime(it.find("pubDate").get_text(strip=True))
                    except Exception:
                        pub = None
                elif it.find("updated"):
                    try:
                        pub = pd.to_datetime(it.find("updated").get_text(strip=True), utc=True, errors="coerce").to_pydatetime()
                    except Exception:
                        pub = None
                out.append((title, link, pub))
            return out or None
        except Exception:
            return None
    if rss_url:
        rss_data = await fetch_and_parse(rss_url)
        if rss_data:
            return rss_data
    try:
        async with session.get(page_url, headers=headers, timeout=timeout, ssl=True) as resp:
            resp.raise_for_status()
            html = await resp.text()
        only_head = SoupStrainer("link")
        soup = BeautifulSoup(html, "html.parser", parse_only=only_head)
        rss_links = [
            l.get("href")
            for l in soup.find_all(
                "link",
                rel=lambda v: v and "alternate" in v,
                type=lambda t: t and ("rss" in t or "atom" in t or "xml" in t),
            )
            if l.get("href")
        ]
        candidates = rss_links + [urljoin(page_url, path) for path in ("/rss", "/feed", "/feeds", "/index.xml")]
        for feed_url in candidates:
            if not feed_url:
                continue
            rss_data = await fetch_and_parse(feed_url)
            if rss_data:
                return rss_data
    except Exception:
        pass
    return None

# =========================
# Scraper (con traducci√≥n por medio)
# =========================
async def scrape_source_async(
    session: aiohttp.ClientSession,
    source: Dict[str, Any],
    terms_by_lang: Dict[str, Dict[str, Any]],
    include_terms_raw: str,
    exclude_terms_raw: str,
    timeout: ClientTimeout,
    ttl_sec: int,
    neg_ttl_sec: int,
    headers: Dict[str, str],
    respect_robots: bool,
    use_date_filter: bool,
    date_field: str,
    start_date: date,
    end_date: date,
) -> List[Dict[str, Any]]:

    name = source.get("name")
    url = source.get("url")
    selector_home = source.get("selector")
    selector_archive = source.get("archive_selector") or selector_home
    base_url = source.get("base_url") or None
    lang = (source.get("lang") or "").lower() or "es"

    html_disabled = bool(source.get("html_disabled", False))
    disable_robots = bool(source.get("disable_robots", False))
    rss_fallback = source.get("rss_fallback")

    if not (name and url):
        return []

    # =========================
    # 1) t√©rminos ya preparados por idioma
    # =========================
    lang_terms = terms_by_lang.get(lang) or {}

    include_re = lang_terms.get("include_re")
    exclude_re = lang_terms.get("exclude_re")
    include_terms_list = lang_terms.get("include_terms", [])
    exclude_terms_list = lang_terms.get("exclude_terms", [])

    def get_relevance(title: str) -> int:
        """
        Score de relevancia:
        - <0  => descartado (por exclusi√≥n)
        -  0  => neutro (no hay t√©rminos o no matchea include_terms)
        - >0  => relevante (m√°s alto = mejor)
        """
        if not title:
            return 0

        # Primero, exclusiones duras
        if exclude_re is not None and exclude_re.search(title):
            return -999

        # Si hay include_re y no matchea, no nos interesa
        if include_re is not None and not include_re.search(title):
            return 0

        # Si pasa los filtros, calculamos score fino
        return compute_relevance_score_from_terms(
            title,
            include_terms_list,
            exclude_terms_list,
        )

    rows: List[Dict[str, Any]] = []

    # Particiona rango en pasados y hoy cuando se filtra por publicaci√≥n
    today = date.today()
    past_days: List[date] = []
    include_today = False

    if use_date_filter and date_field.startswith("Fecha de publicaci√≥n"):
        for d in daterange(start_date, end_date):
            if d < today:
                past_days.append(d)
        # Si el rango incluye hoy, lo marcamos
        if start_date <= today <= end_date:
            include_today = True
    else:
        # Sin filtro de fechas, o filtrando por fecha de extracci√≥n:
        # siempre miramos HOY
        include_today = True

    # =========================
    # 2) ARCHIVOS (pasado)
    # =========================
    if past_days and not html_disabled:
        archive_pattern = source.get("archive_pattern")

        # üîç Si el medio no tiene archive_pattern definido, lo intentamos descubrir
        if not archive_pattern:
            # 1) detector "inteligente" a partir de enlaces con fecha
            detected = await smart_auto_detect_archive_pattern(
                session=session,
                source=source,
                headers=headers,
                timeout=timeout,
                ttl_sec=ttl_sec,
                neg_ttl_sec=neg_ttl_sec,
                respect_robots=(respect_robots and not disable_robots),
            )
            if detected:
                archive_pattern = detected
                source["archive_pattern"] = detected
                st.session_state["db_modified"] = True
            else:
                # 2) fallback con patrones comunes (COMMON_ARCHIVE_PATTERNS)
                detected_common = await auto_detect_archive_pattern_for_source(
                    session=session,
                    source=source,
                    headers=headers,
                    timeout=timeout,
                    ttl_sec=ttl_sec,
                    neg_ttl_sec=neg_ttl_sec,
                    respect_robots=(respect_robots and not disable_robots),
                )
                if detected_common:
                    archive_pattern = detected_common
                    source["archive_pattern"] = detected_common
                    st.session_state["db_modified"] = True

        archive_urls = []

        if is_date_based_pattern(archive_pattern):
            archive_urls = iter_archive_urls_for_dates(source, past_days)
        elif is_page_based_pattern(archive_pattern):
            archive_urls = iter_archive_urls_for_pages(source, max_pages=10)
        elif archive_pattern:
            archive_urls = [archive_pattern]

        if archive_urls and selector_archive:
            for page in archive_urls:
                html = await fetch_html(
                    session,
                    page,
                    headers=headers,
                    timeout=timeout,
                    ttl_sec=ttl_sec,
                    neg_ttl_sec=neg_ttl_sec,
                    respect_robots=(respect_robots and not disable_robots),
                )
                if not html:
                    continue
                try:
                    soup = BeautifulSoup(html, "html.parser")
                    elements = soup.select(selector_archive) if selector_archive else []
                    for el in elements:
                        title = el.get_text(strip=True)
                        href = el.get("href")
                        if not href or not title:
                            continue
                        full_url = absolutize(href, base_url or page)
                        if not full_url:
                            continue
                        if is_relevant(title):
                            raw_dt = extract_time_candidate(el)
                            pub_iso = None
                            if raw_dt:
                                try:
                                    dt = pd.to_datetime(raw_dt, utc=True, errors="coerce")
                                    if pd.notnull(dt):
                                        pub_iso = dt.isoformat()
                                except Exception:
                                    pub_iso = None
                            rows.append({
                                "medio": name,
                                "t√≠tulo": title,
                                "url": full_url,
                                "fecha_extraccion": datetime.now().strftime("%Y-%m-%d"),
                                "publicado": pub_iso,
                                "fuente": "html-archivo",
                            })
                except Exception as e:
                    st.session_state.setdefault("logs", []).append(f"‚ùå {name} ({page}): {e}")

    # =========================
    # 3) HOY (RSS + portada)
    # =========================
    if include_today:
        # a) RSS prioritario: si el medio define rss_fallback lo usamos
        rss_urls_to_try = []
        if rss_fallback:
            rss_urls_to_try.append(rss_fallback)
        rss_urls_to_try.append(url)

        got_rss = False
        for candidate in rss_urls_to_try:
            if rss_fallback and candidate == rss_fallback:
                # Usamos expl√≠citamente el RSS de fallback
                rss = await try_fetch_rss(
                    session=session,
                    page_url=url,        
                    headers=headers,
                    timeout=timeout,
                    rss_url=rss_fallback, 
                )
            else:
                rss = await try_fetch_rss(
                    session=session,
                    page_url=candidate,
                    headers=headers,
                    timeout=timeout,
                )

            if rss:
                got_rss = True
                for title, href, dt in rss:
                    full_url = absolutize(href, base_url or candidate)
                    if not full_url or not title:
                        continue

                    relevance = get_relevance(title)
                    if relevance < 0:
                        continue
                    if include_re is not None and relevance == 0:
                        continue

                    rows.append({
                        "medio": name,
                        "t√≠tulo": title,
                        "url": full_url,
                        "fecha_extraccion": datetime.now().strftime("%Y-%m-%d"),
                        "publicado": (dt.isoformat() if dt else None),
                        "fuente": "rss",
                        "score": relevance,
                    })

        # b) HTML portada
        if selector_home and not html_disabled:
            html = await fetch_html(
                session,
                url,
                headers=headers,
                timeout=timeout,
                ttl_sec=ttl_sec,
                neg_ttl_sec=neg_ttl_sec,
                respect_robots=(respect_robots and not disable_robots),
            )
            if html:
                try:
                    soup = BeautifulSoup(html, "html.parser")
                    elements = soup.select(selector_home) if selector_home else []
                    for el in elements:
                        title = el.get_text(strip=True)
                        href = el.get("href")
                        if not href or not title:
                            continue

                        full_url = absolutize(href, base_url or url)
                        if not full_url:
                            continue

                        # üîé relevancia por t√©rminos
                        relevance = get_relevance(title)
                        if relevance < 0:
                            continue
                        # Si el usuario ha definido t√©rminos de inclusi√≥n, ignoramos score 0
                        if include_re is not None and relevance == 0:
                            continue

                        raw_dt = extract_time_candidate(el)
                        pub_iso = None
                        if raw_dt:
                            try:
                                dt = pd.to_datetime(raw_dt, utc=True, errors="coerce")
                                if pd.notnull(dt):
                                    # üóìÔ∏è filtro de fechas ya en el scraper (si aplica por publicaci√≥n)
                                    if use_date_filter and date_field.startswith("Fecha de publicaci√≥n"):
                                        pub_date = dt.date()
                                        if not (start_date <= pub_date <= end_date):
                                            continue
                                    pub_iso = dt.isoformat()
                            except Exception:
                                pub_iso = None

                        rows.append({
                            "medio": name,
                            "t√≠tulo": title,
                            "url": full_url,
                            "fecha_extraccion": datetime.now().strftime("%Y-%m-%d"),
                            "publicado": pub_iso,
                            "fuente": "html",   # üëà portada, NO "html-archivo"
                            "score": relevance,
                        })
                except Exception as e:
                    st.session_state.setdefault("logs", []).append(f"‚ùå {name} (portada): {e}")

        # c) GOOGLE NEWS (refuerzo, mismo medio, noticias recientes)
        if include_terms_raw and include_terms_raw.strip():
            try:
                gn_items = await fetch_google_news_rss_for_source(
                    session=session,
                    source=source,
                    include_terms_raw=include_terms_raw,
                    headers=headers,
                    timeout=timeout,
                    recent_window_days=7,  # puedes ajustar esta ventana si quieres
                )
            except Exception as e:
                gn_items = None
                st.session_state.setdefault("logs", []).append(
                    f"‚ö†Ô∏è {name}: fallo al consultar Google News RSS ({e})"
                )

            if gn_items:
                base_for_domain = base_url or url
                for title, href, dt in gn_items:
                    full_url = (href or "").strip()
                    if not full_url or not title:
                        continue
                    # Seguridad extra: nos aseguramos de que el link final sea del mismo dominio
                    if base_for_domain and not is_same_domain(full_url, base_for_domain):
                        continue
                    relevance = get_relevance(title)
                    if relevance < 0:
                        continue
                    if include_re is not None and relevance == 0:
                        continue

                    rows.append({
                        "medio": name,
                        "t√≠tulo": title,
                        "url": full_url,
                        "fecha_extraccion": datetime.now().strftime("%Y-%m-%d"),
                        "publicado": (dt.isoformat() if dt else None),
                        "fuente": "gn-rss",
                        "score": relevance,
                    })

    return rows
    
# =========================
# Paralelizaci√≥n
# =========================
async def run_parallel(
    sources: List[Dict[str, Any]],
    terms_by_lang: Dict[str, Dict[str, Any]],
    include_terms_raw: str,
    exclude_terms_raw: str,
    timeout: ClientTimeout,
    concurrency: int,
    ttl_sec: int,
    neg_ttl_sec: int,
    headers: Dict[str, str],
    respect_robots: bool,
    use_date_filter: bool,
    date_field: str,
    start_date: date,
    end_date: date,
    progress_cb=None,
) -> List[Dict[str, Any]]:
    connector = aiohttp.TCPConnector(limit_per_host=concurrency, ssl=None)
    sem = asyncio.Semaphore(concurrency)
    results: List[Dict[str, Any]] = []

    async with aiohttp.ClientSession(connector=connector) as session:

        async def wrapped(src):
            async with sem:
                out = await scrape_source_async(
                    session=session,
                    source=src,
                    terms_by_lang=terms_by_lang,
                    include_terms_raw=include_terms_raw,
                    exclude_terms_raw=exclude_terms_raw,
                    timeout=timeout,
                    ttl_sec=ttl_sec,
                    neg_ttl_sec=neg_ttl_sec,
                    headers=headers,
                    respect_robots=respect_robots,
                    use_date_filter=use_date_filter,
                    date_field=date_field,
                    start_date=start_date,
                    end_date=end_date,
                )
                if progress_cb:
                    progress_cb(src.get("name", "¬ømedio?"), len(out))
                return out

        tasks = [asyncio.create_task(wrapped(s)) for s in sources]
        for coro in asyncio.as_completed(tasks):
            out = await coro
            results.extend(out)

    return results

def dedupe_news(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen = set()
    uniq: List[Dict[str, Any]] = []
    for r in rows:
        key = (norm_text(r.get("t√≠tulo", "")), r.get("url"))
        if key in seen:
            continue
        seen.add(key)
        uniq.append(r)

    by_title: Dict[str, Tuple[int, Dict[str, Any]]] = {}
    for r in uniq:
        t = norm_text(r.get("t√≠tulo", ""))

        score = 0
        if r.get("score") is not None:
            try:
                score += int(r["score"])
            except (TypeError, ValueError):
                pass

        score += (1 if (r.get("fuente", "") or "").startswith("rss") else 0)
        score += (1 if r.get("publicado") else 0)

        cur = by_title.get(t)
        if (not cur) or (score > cur[0]):
            by_title[t] = (score, r)

    return [v[1] for v in by_title.values()]

# =========================
# Estado inicial
# =========================
if "db" not in st.session_state:
    st.session_state.db = load_db()
if not st.session_state.db:
    st.stop()
if "logs" not in st.session_state:
    st.session_state.logs = []
# Mapeo pa√≠s -> continente
COUNTRY_TO_CONTINENT = {
    # Europa
    "Espa√±a": "Europa",
    "Portugal": "Europa",
    "Francia": "Europa",
    "Albania": "Europa",
    "Andorra": "Europa",
    "Alemania": "Europa",
    "Austria": "Europa",
    "B√©lgica": "Europa",
    "Bosnia y Herzegovina": "Europa",
    "Bulgaria": "Europa",
    "Chipre": "Europa",
    "Croacia": "Europa",
    "Dinamarca": "Europa",
    "Rep√∫blica Checa": "Europa",
    "Eslovaquia": "Europa",
    "Eslovenia": "Europa",
    "Estonia": "Europa",
    "Finlandia": "Europa",
    "Grecia": "Europa",
    "Hungr√≠a": "Europa",
    "Irlanda": "Europa",
    "Islandia": "Europa",
    "Italia": "Europa",
    "Isla de Man": "Europa",
    "Kosovo": "Europa",
    "Letonia": "Europa",
    "Luxemburgo": "Europa",
    "Macedonia": "Europa",
    "Malta": "Europa",
    "Moldavia": "Europa",
    "M√≥naco": "Europa",
    "Montenegro": "Europa",
    "Pa√≠ses Bajos": "Europa",
    "Noruega": "Europa",
    "Polonia": "Europa",
    "Reino Unido": "Europa",
    "Ruman√≠a": "Europa",
    "Rusia": "Europa",
    "San Marino": "Europa",
    "Serbia": "Europa",
    "Suecia": "Europa",
    "Suiza": "Europa",
    "Ucrania": "Europa",
    # √Åfrica
    "Algeria": "√Åfrica",
    "Marruecos": "√Åfrica",
    "Argelia": "√Åfrica",
    "Burundi": "√Åfrica",
    "Camer√∫n": "√Åfrica",
    "Cabo Verde": "√Åfrica",
    "Rep√∫blica Democr√°tica del Congo": "√Åfrica",
    "Yibuti": "√Åfrica",
    "Egipto": "√Åfrica",
    "Eritrea": "√Åfrica",
    "Etiop√≠a": "√Åfrica",
    "Gambia": "√Åfrica",
    "Guinea": "√Åfrica",
    "Kenia": "√Åfrica",
    "Liberia": "√Åfrica",
    "Madagascar": "√Åfrica",
    "Malaui": "√Åfrica",
    "Mauricio": "√Åfrica",
    "Namibia": "√Åfrica",
    "N√≠ger": "√Åfrica",
    "Nigeria": "√Åfrica",
    "Senegal": "√Åfrica",
    "Somalia": "√Åfrica",
    "Sud√°frica": "√Åfrica",
    "Togo": "√Åfrica",
    "T√∫nez": "√Åfrica",
    "Uganda": "√Åfrica",
    "Zambia": "√Åfrica",
    "Zimbabue": "√Åfrica",
    # Am√©rica
    "Barbados": "Am√©rica",
    "Belice": "Am√©rica",
    "Bermudas": "Am√©rica",
    "Bolivia": "Am√©rica",
    "Brasil": "Am√©rica",
    "Canad√°": "Am√©rica",
    "Chile": "Am√©rica",
    "Colombia": "Am√©rica",
    "Costa Rica": "Am√©rica",
    "Cuba": "Am√©rica",
    "Dominica": "Am√©rica",
    "Rep√∫blica Dominicana": "Am√©rica",
    "Ecuador": "Am√©rica",
    "Argentina": "Am√©rica",
    "Bahamas": "Am√©rica",
    "Estados Unidos": "Am√©rica",
    "Aruba": "Am√©rica",
    "Guatemala": "Am√©rica",
    "Guyana": "Am√©rica",
    "Hait√≠": "Am√©rica",
    "Jamaica": "Am√©rica",
    "M√©xico": "Am√©rica",
    "Nicaragua": "Am√©rica",
    "Panam√°": "Am√©rica",
    "Per√∫": "Am√©rica",
    "Uruguay": "Am√©rica",
    "Venezuela": "Am√©rica",
    "San Crist√≥bal y Nieves": "Am√©rica",
    "Santa Elena": "Am√©rica",
    "Surinam": "Am√©rica",
    "Martinica": "Am√©rica",
    # Asia
    "Afghanistan": "Asia",
    "Arabia Saud√≠": "Asia",
    "Armenia": "Asia",
    "Azerbaiy√°n": "Asia",
    "Bar√©in": "Asia",
    "Banglad√©s": "Asia",
    "Birmania": "Asia",
    "Corea del Sur": "Asia",
    "Jap√≥n": "Asia",
    "Palestina": "Asia",
    "Israel": "Asia",
    "India": "Asia",
    "Indonesia": "Asia",
    "Ir√°n": "Asia",
    "Hong Kong": "Asia",
    "Jordania": "Asia",
    "Kazajist√°n": "Asia",
    "Kuwait": "Asia",
    "Kirguist√°n": "Asia",
    "Malasia": "Asia",
    "Maldivas": "Asia",
    "Macao": "Asia",
    "Mongolia": "Asia",
    "Nepal": "Asia",
    "Pakist√°n": "Asia",
    "Palestina": "Asia",
    "Catar": "Asia",
    "Sri Lanka": "Asia",
    "Siria": "Asia",
    "Tailandia": "Asia",
    "Timor Oriental": "Asia",
    "Turqu√≠a": "Asia",
    "Vietnam": "Asia",
    "Yemen": "Asia",
    # Ocean√≠a
    "Australia": "Ocean√≠a",
    "Polinesia Francesa": "Ocean√≠a",
    "Islas Caim√°n": "Ocean√≠a",
    "Guam": "Ocean√≠a",
    "Samoa": "Ocean√≠a",
    "Nueva Zelanda": "Ocean√≠a",
}

def get_continent_for_country(country: str) -> str:
    return COUNTRY_TO_CONTINENT.get(country, "Otros")

# Defaults filtro temporal y traducci√≥n
if "use_date_filter" not in st.session_state:
    st.session_state.use_date_filter = False
if "date_range" not in st.session_state:
    st.session_state.date_range = (date.today() - timedelta(days=7), date.today())
if "date_field" not in st.session_state:
    st.session_state.date_field = "Fecha de publicaci√≥n (recomendado)"
if "include_na_pub" not in st.session_state:
    st.session_state.include_na_pub = True
if "translate_per_source" not in st.session_state:
    st.session_state.translate_per_source = True

# =========================
# Sidebar: gesti√≥n BD
# =========================
all_countries = sorted(st.session_state.db.keys())

continent_to_countries = {}
for c in all_countries:
    cont = get_continent_for_country(c)
    continent_to_countries.setdefault(cont, []).append(c)

all_continents = sorted(continent_to_countries.keys())

st.sidebar.header("üóÇÔ∏è LISTADO DE MEDIOS")

sidebar_cont = st.sidebar.selectbox("Continente", all_continents, key="sidebar_continent")
countries_in_sidebar_cont = sorted(continent_to_countries.get(sidebar_cont, []))

default_country = "Espa√±a" if "Espa√±a" in countries_in_sidebar_cont else (countries_in_sidebar_cont[0] if countries_in_sidebar_cont else "")
country = st.sidebar.selectbox(
    "Pa√≠s",
    countries_in_sidebar_cont,
    index=countries_in_sidebar_cont.index(default_country) if default_country in countries_in_sidebar_cont else 0,
    key="sidebar_country",
)

with st.sidebar.expander(
    f"üìú Medios en {country} ({len(st.session_state.db.get(country, []))})",
    expanded=True
):
    medios = st.session_state.db.get(country, [])
    if not medios:
        st.caption("Sin medios registrados.")
    else:
        medios = sorted(medios, key=lambda x: x.get("name", ""))
        cols = st.columns(2)
        for i, s in enumerate(medios):
            with cols[i % 2]:
                st.link_button(s["name"], s["url"], use_container_width=True)

st.sidebar.markdown("---")

# =========================
# Controles de b√∫squeda
# =========================
all_countries = sorted(st.session_state.db.keys())

# Construimos continentes a partir de lo que haya en el JSON
continent_to_countries = {}
for c in all_countries:
    cont = get_continent_for_country(c)
    continent_to_countries.setdefault(cont, []).append(c)

all_continents = sorted(continent_to_countries.keys())

# 1) selector de continente
selected_continent = st.selectbox(
    "Continente",
    all_continents,
    key="search_continent",
)

# 2) selector de pa√≠s (filtrado por continente)
countries_in_cont = sorted(continent_to_countries.get(selected_continent, []))

# si el pa√≠s guardado en sesi√≥n no est√° en este continente, ponemos el primero
default_country_for_cont = (
    st.session_state.get("search_country")
    if st.session_state.get("search_country") in countries_in_cont
    else (countries_in_cont[0] if countries_in_cont else "")
)

search_country = st.selectbox(
    "Pa√≠s a buscar",
    countries_in_cont,
    index=countries_in_cont.index(default_country_for_cont) if default_country_for_cont in countries_in_cont else 0,
    key="search_country",
)

# (2) Texto y opciones b√°sicas
colA, colB, colC = st.columns([2, 2, 1])
with colA:
    include_terms = st.text_input(
        "T√©rminos a incluir:",
        placeholder='Ej.: "Cambio clim√°tico", flotilla, Gobierno',
    )
with colB:
    exclude_terms = st.text_input("T√©rminos a excluir (opcional):", placeholder='Ej.: "Guerra civil", subvenci√≥n, militar')
with colC:
    whole_words = st.checkbox("Coincidencia por palabra", value=False, help="Se aplicar√° solo en idiomas latinos autom√°ticamente.")
    ignore_case = st.checkbox("Ignorar may√∫sc./min√∫sc.", value=True)

# (3) Filtro temporal
with st.expander("üóìÔ∏è Filtro temporal"):
    st.checkbox("Filtrar por periodo de fechas", key="use_date_filter", value=st.session_state.use_date_filter)

    if st.session_state.use_date_filter:
        st.date_input(
            "Periodo (inicio y fin)",
            key="date_range",
            value=st.session_state.date_range,
            format="DD-MM-YYYY",
        )

        st.selectbox(
            "Campo de fecha a usar",
            ["Fecha de publicaci√≥n (recomendado)", "Fecha de extracci√≥n"],
            key="date_field",
            index=0 if st.session_state.date_field.startswith("Fecha de publicaci√≥n") else 1,
        )

        st.checkbox(
            "Incluir noticias sin fecha de publicaci√≥n",
            key="include_na_pub",
            value=st.session_state.include_na_pub,
            help="S√≥lo aplica cuando filtras por 'Fecha de publicaci√≥n'.",
        )

# (3b) Traducci√≥n por medio
with st.expander("üåê Traducci√≥n de t√©rminos"):
    st.checkbox(
        "Traducir t√©rminos por idioma de cada medio (Google Translate)",
        key="translate_per_source",
        help="Mantiene el t√©rmino original y a√±ade su traducci√≥n. Si usas comillas, tambi√©n se hace coincidencia exacta."
    )
    st.caption("La traducci√≥n se aplica al pulsar **Buscar** para evitar llamadas innecesarias. Se cachea por texto+idioma.")

# Leer/normalizar valores del filtro
use_date_filter = bool(st.session_state.get("use_date_filter", False))
dr = st.session_state.get("date_range", None)
if isinstance(dr, (list, tuple)) and len(dr) == 2:
    start_date, end_date = dr
else:
    d = dr or (date.today(),)
    d = d[0] if isinstance(d, (list, tuple)) else d
    start_date, end_date = d, d
date_field = st.session_state.get("date_field", "Fecha de publicaci√≥n (recomendado)")
include_na_pub = bool(st.session_state.get("include_na_pub", True))
translate_per_source = bool(st.session_state.get("translate_per_source", True))

# (4) Opciones avanzadas
with st.expander("‚öôÔ∏è Opciones avanzadas"):
    current_sources = st.session_state.db.get(search_country, [])
    preselect = [s["name"] for s in current_sources]
    selected_names = st.multiselect(
        f"Medios de {search_country} a incluir",
        options=preselect,
        default=preselect,
        key=f"sources_{search_country}",
    )
    timeout_sec = st.slider("Timeout por petici√≥n (seg.)", min_value=5, max_value=40, value=20, step=1)
    concurrency = st.slider("Concurrencia (simult√°neos)", min_value=2, max_value=20, value=8, step=1)
    ttl_sec = st.slider("TTL cach√© positiva (seg.)", min_value=60, max_value=3600, value=900, step=30)
    neg_ttl_sec = st.slider("TTL cach√© negativa (seg.)", min_value=5, max_value=120, value=30, step=5)
    respect_robots = st.checkbox("Respetar robots.txt (beta)", value=True)
    show_log = st.checkbox("Mostrar LOG al finalizar", value=True)

# =========================
# Acci√≥n: Buscar
# =========================
def add_log_line(name: str, n: int):
    st.session_state.logs.append(f"‚úÖ {name}: {n} resultados")

if st.button("üîç Buscar en pa√≠s seleccionado", type="primary"):
    st.session_state.logs = []

    # Fuentes a consultar
    sources = [s for s in st.session_state.db.get(search_country, []) if s["name"] in selected_names]

    if not sources:
        st.warning(f"No hay medios seleccionados para {search_country}.")
    else:
        start = time.time()
        headers = build_headers(search_country)
        timeout = ClientTimeout(
            total=timeout_sec,
            connect=min(10, timeout_sec),
            sock_connect=min(10, timeout_sec),
            sock_read=min(max(5, timeout_sec - 5), timeout_sec),
        )
        with st.spinner("Buscando en medios‚Ä¶"):
            # 2) ejecuto todo en un solo loop
            async def main():
                async with aiohttp.ClientSession() as session:
                    # 2a) preparar t√©rminos por idioma
                    needed_langs = sorted(set((s.get("lang") or "es").lower() for s in sources))
                    terms_by_lang = await prepare_terms_per_language(
                        session=session,
                        include_terms_raw=include_terms,
                        exclude_terms_raw=exclude_terms,
                        languages=needed_langs,
                        user_whole_words=whole_words,
                        ignore_case=ignore_case,
                        enable_translation=translate_per_source,
                    )
                    # 2b) lanzar scrapers en paralelo
                    rows_all = await run_parallel(
                        sources=sources,
                        terms_by_lang=terms_by_lang,
                        include_terms_raw=include_terms,
                        exclude_terms_raw=exclude_terms,
                        timeout=timeout,
                        concurrency=concurrency,
                        ttl_sec=ttl_sec,
                        neg_ttl_sec=neg_ttl_sec,
                        headers=headers,
                        respect_robots=respect_robots,
                        use_date_filter=use_date_filter,
                        date_field=date_field,
                        start_date=start_date,
                        end_date=end_date,
                        progress_cb=add_log_line,
                    )
                    return rows_all
        
            rows_all: List[Dict[str, Any]] = asyncio.run(main())

        elapsed = time.time() - start
        rows_all = dedupe_news(rows_all)

        if not rows_all:
            st.warning("üö´ No se encontraron noticias con los criterios dados.")
        else:
            df = pd.DataFrame(rows_all)

            # Filtrado temporal post (seguridad)
            if use_date_filter:
                df["_pub_dt"] = pd.to_datetime(df.get("publicado"), utc=True, errors="coerce")
                df["_ext_dt"] = pd.to_datetime(df.get("fecha_extraccion"), format="%Y-%m-%d", errors="coerce")

                start_ts_utc = pd.Timestamp(start_date).tz_localize("UTC")
                end_ts_utc = (pd.Timestamp(end_date).tz_localize("UTC") + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1))
                start_ts_naive = pd.Timestamp(start_date)
                end_ts_naive = (pd.Timestamp(end_date) + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1))

                if date_field.startswith("Fecha de publicaci√≥n"):
                    mask_pub = df["_pub_dt"].between(start_ts_utc, end_ts_utc, inclusive="both")
                    if include_na_pub:
                        df = df[mask_pub | df["_pub_dt"].isna()].copy()
                    else:
                        df = df[mask_pub].copy()
                else:
                    mask_ext = df["_ext_dt"].between(start_ts_naive, end_ts_naive, inclusive="both")
                    df = df[mask_ext].copy()

            if "publicado" in df.columns:
                df["_dt"] = pd.to_datetime(df["publicado"], utc=True, errors="coerce")
            else:
                df["_dt"] = pd.NaT
            if "publicado" in df.columns:
                df["_dt"] = pd.to_datetime(df["publicado"], utc=True, errors="coerce")
            else:
                df["_dt"] = pd.NaT
            
            if "score" in df.columns:
                df = df.sort_values(
                    ["score", "_dt", "fecha_extraccion"],
                    ascending=[False, False, False],
                )
            else:
                df = df.sort_values(
                    ["_dt", "fecha_extraccion"],
                    ascending=[False, False],
                )
            
            df = df.drop(columns=["_dt"], errors="ignore")

            st.success(f"‚úÖ {len(df)} noticias encontradas en {search_country} (en {elapsed:.1f}s).")
            st.dataframe(df, use_container_width=True, hide_index=True)

            # üíæ OPCIONAL: guardar patrones de hemeroteca detectados autom√°ticamente
            if st.session_state.get("db_modified"):
                save_db(st.session_state.db)
                st.session_state["db_modified"] = False
                st.success("Patrones de hemeroteca nuevos guardados autom√°ticamente ‚úÖ")


            st.write("### Resultados")
            for _, row in df.iterrows():
                fuente = row.get("fuente") or "-"
                publicado = row.get("publicado") or "‚Äî"
                st.markdown(
                    f"""
                    <div style="border:1px solid #e5e7eb;border-radius:10px;padding:12px 14px;margin-bottom:10px;">
                      <p style="margin:0;color:#6b7280;font-size:13px;">üì∞ <strong>{row['medio']}</strong> ¬∑ <span style="background:#eef2ff;padding:2px 6px;border-radius:6px;">{(fuente or '').upper()}</span></p>
                      <p style="margin:6px 0;">
                        <a href="{row['url']}" target="_blank" rel="noopener noreferrer" referrerpolicy="no-referrer" style="font-size:16px;text-decoration:none;">
                          {row['t√≠tulo']}
                        </a>
                      </p>
                      <p style="margin:0;color:#6b7280;font-size:12px;">üìÖ Publicado: {publicado} ¬∑ Extra√≠do: {row['fecha_extraccion']}</p>
                    </div>
                    """,
                    unsafe_allow_html=True,
                )

            csv = df.to_csv(index=False).encode("utf-8")
            st.download_button(
                "üì• Descargar CSV",
                csv,
                file_name=f"noticias_{search_country}_{datetime.now().date()}.csv",
                mime="text/csv",
                use_container_width=True,
            )

        if show_log and st.session_state.logs:
            st.markdown("### LOG")
            st.code("\n".join(st.session_state.logs), language="text")
